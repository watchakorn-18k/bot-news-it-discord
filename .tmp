Serverless strategies for streaming LLM responses